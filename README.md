# Fuel-Constrained DQN on LunarLander-v2

This repository contains a Deep Q-Network (DQN) agent trained on the LunarLander-v2 environment from [Gymnasium](https://gymnasium.farama.org/environments/box2d/lunar_lander/), with a custom reward shaping mechanism that penalizes excessive fuel usage. The goal is to promote efficient landings while maintaining high success rates.

## ðŸš€ Overview

Traditional reinforcement learning agents trained on LunarLander often prioritize landing success at the cost of realismâ€”expending large amounts of fuel. This project modifies the reward function to:

- Penalize excess main engine and side engine usage.
- Encourage fuel-efficient control strategies.
- Maintain a balance between landing success and fuel conservation.

## ðŸ§  Key Components

- **Custom DQN Agent**: Fully implemented from scratch using PyTorch.
- **Modified Reward Function**: Adds negative rewards based on engine thrust.
- **Training & Evaluation Scripts**: Easily train and test performance across episodes.
- **Performance Visualization**: Includes plots comparing baseline and fuel-constrained agents.

## ðŸ“¦ Project Structure

```bash
fuel-constrained-dqn-lunarlander/
â”œâ”€â”€ dqn_agent.py           # Custom DQN implementation
â”œâ”€â”€ train.py               # Training script
â”œâ”€â”€ evaluate.py            # Evaluation script
â”œâ”€â”€ reward_wrapper.py      # Reward shaping for fuel penalty
â”œâ”€â”€ utils.py               # Logging, saving, etc.
â”œâ”€â”€ plots/                 # Training and evaluation graphs
â”œâ”€â”€ saved_models/          # Model checkpoints
â””â”€â”€ README.md
